import os
import openai
import logging
import pinecone
import importlib
import uuid
import requests
import tempfile
import PyPDF2
import docx
from typing import List, Dict, Optional, Any, Type, Union
import numpy as np
from enum import Enum
from abc import ABC, abstractmethod
import httpx
import asyncio
from fastapi import HTTPException, status
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from openai import AsyncOpenAI
from datetime import datetime

# Import settings instance from centralized config module
from app.config.settings import settings

# Configure logging
logger = logging.getLogger(__name__)

# Initialize OpenAI client using settings
openai_client = AsyncOpenAI(
    api_key=settings.OPENAI_API_KEY,
    base_url=settings.OPENAI_API_BASE,  # Use the full base URL from settings
)
logger.info(f"OpenAI client initialized with base URL: {settings.OPENAI_API_BASE}")

# Embedding Provider Enum
class EmbeddingProvider(str, Enum):
    OPENAI = "openai"
    PINECONE = "pinecone"
    # Add other providers as needed
    # HUGGINGFACE = "huggingface"
    # COHERE = "cohere"


# Abstract Embedding Provider Interface
class EmbeddingProviderInterface(ABC):
    """Abstract base class for embedding providers"""

    @abstractmethod
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        pass

    @abstractmethod
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        """Return the dimension of embeddings generated by this provider"""
        pass


# OpenAI Embedding Provider Implementation
class OpenAIEmbeddingProvider(EmbeddingProviderInterface):
    """OpenAI embedding provider implementation"""

    def __init__(self, model: str = settings.EMBEDDING_MODEL, dimension: int = settings.EMBEDDING_DIMENSION):
        """Initialize with OpenAI configuration"""
        self.model = model
        self.model_dimension = dimension # Store the target dimension
        self.api_key = settings.OPENAI_API_KEY
        self.api_base = settings.OPENAI_API_BASE

        if not self.api_key:
            logger.error("OpenAI API key is not configured!")
            raise ValueError(
                "OpenAI API key not configured. Cannot generate embeddings."
            )

        # Set the API key and base URL for the client
        openai.api_key = self.api_key
        openai.base_url = self.api_base

        # Ensure embedding endpoint has the correct format with a trailing slash
        self.embeddings_url = f"{self.api_base.rstrip('/')}/embeddings"
        logger.info(f"OpenAI embedding provider initialized with model: {self.model}")
        logger.info(f"Using API base URL: {self.api_base}")
        logger.info(f"Using embeddings URL: {self.embeddings_url}")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI API"""
        if not texts:
            return []

        try:
            logger.info(
                f"Generating embeddings for {len(texts)} chunks using {self.model}..."
            )

            # Make sure we have the right URL format
            if not self.api_base.endswith('/'):
                base_url = f"{self.api_base}/"
            else:
                base_url = self.api_base
                
            # Use batching to avoid timeouts on large requests
            batch_size = 50  # OpenAI recommends smaller batches
            all_embeddings = []
            
            # Process in batches
            for i in range(0, len(texts), batch_size):
                batch = texts[i:min(i + batch_size, len(texts))]
                logger.info(f"Processing batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")
                
                # Implement retry logic
                max_retries = 3
                retry_delay = 1.0
                
                for attempt in range(max_retries):
                    try:
                        # Use the AsyncOpenAI client
                        response = await openai_client.embeddings.create(
                            input=batch,
                            model=self.model,
                            dimensions=self.model_dimension # Request specific dimension
                        )
                        
                        batch_embeddings = [item.embedding for item in response.data]
                        all_embeddings.extend(batch_embeddings)
                        
                        # Successfully processed this batch
                        break
                        
                    except (openai.RateLimitError, openai.APITimeoutError) as rate_err:
                        # Handle rate limiting with exponential backoff
                        if attempt < max_retries - 1:
                            backoff_time = retry_delay * (2 ** attempt)
                            logger.warning(f"Rate limit encountered. Retrying batch after {backoff_time:.1f}s delay. Attempt {attempt+1}/{max_retries}")
                            await asyncio.sleep(backoff_time)
                        else:
                            logger.error(f"Failed to generate embeddings after {max_retries} attempts: {str(rate_err)}")
                            raise
                
                # Small delay between batches to avoid rate limits
                if i + batch_size < len(texts):
                    await asyncio.sleep(0.5)
            
            logger.info(f"Successfully generated {len(all_embeddings)} embeddings.")

            # Resize embeddings to match Pinecone's dimension if needed
            if all_embeddings and len(all_embeddings[0]) != self.model_dimension:
                logger.warning(
                    f"Resizing embeddings from {len(all_embeddings[0])} to {self.model_dimension} dimensions to match Pinecone"
                )
                resized_embeddings = []
                for emb in all_embeddings:
                    if len(emb) > self.model_dimension:
                        # Truncate to first EMBEDDING_DIMENSION elements
                        resized_embeddings.append(emb[: self.model_dimension])
                    else:
                        # Pad with zeros if needed (shouldn't happen with OpenAI models)
                        resized_embeddings.append(
                            emb + [0.0] * (self.model_dimension - len(emb))
                        )
                return resized_embeddings

            return all_embeddings

        except (openai.RateLimitError, openai.APIStatusError) as api_err:
            logger.error(
                f"OpenAI API error during batch embedding generation: {type(api_err).__name__}: {str(api_err)}"
            )
            if "insufficient_quota" in str(api_err).lower():
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="OpenAI API quota exceeded. Please check your subscription.",
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Embedding service temporarily unavailable. Please try again later.",
                )
        except Exception as e:
            logger.error(f"Unexpected error during batch embedding generation: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate embeddings: {str(e)}",
            )

    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        if not text:
            return []

        try:
            # Use the AsyncOpenAI client
            response = await openai_client.embeddings.create(
                input=[text], 
                model=self.model,
                dimensions=self.model_dimension # Request specific dimension
            )
            embedding = response.data[0].embedding

            # Resize embedding to match Pinecone's dimension if needed
            # This check is now more of a safeguard, as we requested the specific dimension.
            if len(embedding) != self.model_dimension:
                logger.warning(
                    f"Resizing embedding from {len(embedding)} to {self.model_dimension} dimensions (requested: {self.model_dimension})"
                )
                if len(embedding) > self.model_dimension:
                    # Truncate
                    return embedding[: self.model_dimension]
                else:
                    # Pad with zeros
                    return embedding + [0.0] * (
                        self.model_dimension - len(embedding)
                    )

            return embedding

        except (openai.RateLimitError, openai.APIStatusError) as api_err:
            logger.error(
                f"OpenAI API error during embedding generation: {type(api_err).__name__}: {str(api_err)}"
            )
            if "insufficient_quota" in str(api_err).lower():
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="OpenAI API quota exceeded. Please check your subscription.",
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Embedding service temporarily unavailable. Please try again later.",
                )
        except openai.APIConnectionError as conn_err:
            logger.error(f"OpenAI API connection error: {conn_err}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Could not connect to embedding service. Please try again later.",
            )
        except Exception as e:
            logger.error(
                f"Unexpected error generating embedding: {str(e)}", exc_info=True
            )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="An unexpected error occurred while generating embeddings.",
            )

    @property
    def dimension(self) -> int:
        """Return dimension of embeddings for this provider, set to match Pinecone"""
        # This should reflect the dimension we are requesting and/or ensuring.
        return self.model_dimension


# Factory for creating embedding providers
class EmbeddingProviderFactory:
    """Factory to create the appropriate embedding provider"""

    @staticmethod
    def get_provider(
        provider_type: EmbeddingProvider = None,
    ) -> EmbeddingProviderInterface:
        """Get the appropriate embedding provider based on configuration"""
        # Use provided type or fall back to env variable
        if provider_type is None:
            provider_type = settings.EMBEDDING_PROVIDER

        try:
            provider_type = EmbeddingProvider(provider_type)
        except ValueError:
            logger.warning(
                f"Unknown embedding provider: {provider_type}. Falling back to OpenAI."
            )
            provider_type = EmbeddingProvider.OPENAI

        # Return appropriate provider
        if provider_type == EmbeddingProvider.OPENAI:
            return OpenAIEmbeddingProvider(model=settings.EMBEDDING_MODEL, dimension=settings.EMBEDDING_DIMENSION)
        elif provider_type == EmbeddingProvider.PINECONE:
            # TODO: Implement Pinecone embedding provider
            # For now, fall back to OpenAI as Pinecone doesn't have its own embeddings yet
            logger.warning(
                "Pinecone embedding provider not yet implemented. Using OpenAI."
            )
            return OpenAIEmbeddingProvider(model=settings.EMBEDDING_MODEL, dimension=settings.EMBEDDING_DIMENSION)
        else:
            # Default to OpenAI
            return OpenAIEmbeddingProvider(model=settings.EMBEDDING_MODEL, dimension=settings.EMBEDDING_DIMENSION)


def extract_text_from_file(file_url: str, file_name: str) -> str:
    """Extract text from uploaded documents (PDF, DOCX, TXT)."""
    try:
        # Download file from S3
        response = requests.get(file_url)
        if response.status_code != 200:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to download file from storage",
            )

        # Create temp file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(response.content)
            temp_file_path = temp_file.name

        # Extract text based on file type
        file_extension = file_name.split(".")[-1].lower()

        if file_extension == "pdf":
            # Process PDF
            text = ""
            with open(temp_file_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text() or ""  # Handle None return
                    text += page_text + "\n"

        elif file_extension == "docx":
            # Process DOCX
            doc = docx.Document(temp_file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])

        elif file_extension == "txt":
            # Process TXT
            with open(temp_file_path, "r", encoding="utf-8", errors="replace") as file:
                text = file.read()

        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Unsupported file type: {file_extension}",
            )

        # Clean up temp file
        os.unlink(temp_file_path)
        
        if not text or text.isspace():
            logger.warning(f"No text extracted from {file_name}")
            return f"No readable text found in {file_name}"
            
        return text

    except Exception as e:
        if isinstance(e, HTTPException):
            raise e
        logger.error(f"Failed to extract text: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to extract text: {str(e)}",
        )


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks with semantic coherence."""
    # Sanitize input
    if not text or text.isspace():
        logger.warning("Empty or whitespace-only text provided to chunk_text")
        return []
        
    text = text.strip()
    
    # If text is smaller than chunk_size, return it as a single chunk
    if len(text) <= chunk_size:
        return [text]
        
    # Initialize chunks list
    chunks = []
    
    # Step 1: Split by paragraph breaks first to preserve semantic units
    paragraphs = [p for p in text.split("\n\n") if p and not p.isspace()]
    
    if not paragraphs:
        # If no paragraphs found, try splitting by newlines
        paragraphs = [p for p in text.split("\n") if p and not p.isspace()]
        
    if not paragraphs:
        # If still no paragraphs, fall back to sentence-based splitting
        logger.warning("No paragraphs found, falling back to sentence-based chunking")
        import re
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = ""
        for sentence in sentences:
            # If adding this sentence would exceed chunk_size, start a new chunk
            if len(current_chunk) + len(sentence) + 1 > chunk_size and current_chunk:
                chunks.append(current_chunk)
                # Start new chunk with overlap
                words = current_chunk.split()
                if len(words) > 0:
                    # Calculate how many words to keep for overlap
                    overlap_word_count = min(len(words), overlap // 5)  # Rough approximation
                    current_chunk = " ".join(words[-overlap_word_count:]) + " " + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += " "
                current_chunk += sentence
                
        if current_chunk:  # Add the last chunk
            chunks.append(current_chunk)
    else:
        # Paragraph-based chunking
        current_chunk = ""
        for i, para in enumerate(paragraphs):
            # If this paragraph alone exceeds chunk_size, we need to split it
            if len(para) > chunk_size:
                # First, add any accumulated text as a chunk
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = ""
                
                # For long paragraphs, try to split by sentences
                import re
                sentences = re.split(r'(?<=[.!?])\s+', para)
                
                sentence_chunk = ""
                for sentence in sentences:
                    if len(sentence_chunk) + len(sentence) + 1 <= chunk_size:
                        if sentence_chunk:
                            sentence_chunk += " "
                        sentence_chunk += sentence
                    else:
                        if sentence_chunk:
                            chunks.append(sentence_chunk)
                            
                            # Calculate overlap
                            words = sentence_chunk.split()
                            overlap_word_count = min(len(words), overlap // 5)
                            sentence_chunk = " ".join(words[-overlap_word_count:])
                            
                            if sentence_chunk:
                                sentence_chunk += " "
                            sentence_chunk += sentence
                        else:
                            # Handle case where a single sentence is longer than chunk_size
                            chunks.append(sentence[:chunk_size])
                            sentence_chunk = sentence[max(0, chunk_size - overlap):]
                
                if sentence_chunk:
                    chunks.append(sentence_chunk)
            else:
                # Check if adding this paragraph would exceed chunk_size
                if len(current_chunk) + len(para) + 2 > chunk_size and current_chunk:
                    chunks.append(current_chunk)
                    
                    # Calculate how many characters to repeat for overlap
                    overlap_chars = min(len(current_chunk), overlap)
                    current_chunk = current_chunk[-overlap_chars:] if overlap_chars > 0 else ""
                    
                    # If the overlap breaks in the middle of a word, adjust
                    if current_chunk and not current_chunk[0].isspace() and len(current_chunk) < len(paragraphs[i-1]):
                        # Find the first space and trim to start at a word boundary
                        space_pos = current_chunk.find(' ')
                        if space_pos > 0:
                            current_chunk = current_chunk[space_pos+1:]
                    
                    # Add paragraph to the new chunk
                    if current_chunk:
                        current_chunk += "\n\n"
                    current_chunk += para
                else:
                    if current_chunk:
                        current_chunk += "\n\n"
                    current_chunk += para
        
        # Add the last chunk if there's anything left
        if current_chunk:
            chunks.append(current_chunk)
    
    # If we somehow still have no chunks, fall back to a simple character-based chunking
    if not chunks:
        logger.warning("Falling back to character-based chunking as last resort")
        for i in range(0, len(text), chunk_size - overlap):
            end = min(i + chunk_size, len(text))
            chunks.append(text[i:end])
    
    # Log results
    logger.info(f"Split text into {len(chunks)} chunks (avg size: {sum(len(c) for c in chunks) / max(1, len(chunks)):.0f} chars)")
    
    return chunks


class EmbeddingService:
    """Service for generating embeddings from text."""

    def __init__(self):
        """Initialize the embedding service using settings."""
        self.model = settings.EMBEDDING_MODEL
        self._dimension = settings.EMBEDDING_DIMENSION
        self.provider = settings.EMBEDDING_PROVIDER
        self.openai_client = openai_client  # Use the shared client

        logger.info(f"Embedding service initialized with model: {self.model}")
        if self.provider == "openai":
            logger.info(f"Using embedding endpoint: {settings.OPENAI_EMBEDDINGS_URL}")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts.

        This method delegates to the underlying provider implementation.
        It includes retry logic, error handling, and performance optimizations.
        """
        if not texts:
            logger.debug("Empty texts list provided to generate_embeddings")
            return []

        start_time = datetime.now()
        logger.info(f"Generating embeddings for {len(texts)} texts")

        try:
            embedding_provider = EmbeddingProviderFactory.get_provider()
            
            # Generate embeddings in batches to avoid rate limits and timeouts
            batch_size = 20  # Adjust based on your API rate limits
            embeddings = []
            
            # Create a simple progress tracker
            total_batches = (len(texts) + batch_size - 1) // batch_size
            processed_chunks = 0
            failed_chunks = 0
            
            # Process batches
            for i in range(0, len(texts), batch_size):
                batch = texts[i:min(i + batch_size, len(texts))]
                batch_num = i // batch_size + 1
                logger.info(f"Processing batch {batch_num} of {total_batches} ({len(batch)} chunks)")
                
                try:
                    # Process this batch
                    batch_embeddings = await embedding_provider.generate_embeddings(batch)
                    embeddings.extend(batch_embeddings)
                    processed_chunks += len(batch)
                    
                    # Calculate and log progress
                    progress_percent = (processed_chunks / len(texts)) * 100
                    logger.info(f"Embedding progress: {progress_percent:.1f}% ({processed_chunks}/{len(texts)})")
                    
                except Exception as batch_error:
                    # Log the error but continue with next batch
                    logger.error(f"Error in batch {batch_num}: {str(batch_error)}")
                    failed_chunks += len(batch)
                    
                    # Create dummy embeddings of the right dimension for failed chunks
                    # This ensures we maintain the correct count even with failures
                    dummy_dim = self._dimension
                    dummy_embeddings = [[0.0] * dummy_dim for _ in range(len(batch))]
                    embeddings.extend(dummy_embeddings)
                    
                    logger.warning(f"Added {len(batch)} empty embeddings for failed batch to maintain count")
            
            # Verify we have the expected number of embeddings
            if len(embeddings) != len(texts):
                logger.warning(
                    f"Embedding count mismatch: expected {len(texts)}, got {len(embeddings)}"
                )
                # Try to handle the mismatch - pad with zeros if we have fewer embeddings
                if len(embeddings) < len(texts):
                    dummy_dim = self._dimension
                    missing_count = len(texts) - len(embeddings)
                    dummy_embeddings = [[0.0] * dummy_dim for _ in range(missing_count)]
                    embeddings.extend(dummy_embeddings)
                    logger.warning(f"Added {missing_count} empty embeddings to match count")
                elif len(embeddings) > len(texts):
                    # Truncate if we somehow have too many
                    embeddings = embeddings[:len(texts)]
                    logger.warning(f"Truncated embedding list to match expected count")
            
            # Calculate and log performance metrics
            elapsed_time = (datetime.now() - start_time).total_seconds()
            chunks_per_second = len(texts) / max(0.1, elapsed_time)
            
            logger.info(
                f"Generated {len(embeddings)} embeddings in {elapsed_time:.2f} seconds "
                f"({chunks_per_second:.1f} chunks/sec, {failed_chunks} failures)"
            )
            
            return embeddings

        except HTTPException as http_exc:
            # Re-raise HTTP exceptions
            logger.error(f"HTTP error during embedding generation: {str(http_exc)}")
            raise http_exc
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to generate embeddings: {str(e)}",
            )

    async def generate_single_embedding(self, text: str) -> List[float]:
        """
        Generate an embedding for a single text string.

        Args:
            text: Text string to embed

        Returns:
            Embedding vector for the input text
        """
        try:
            logger.info("Generating single embedding")
            embedding_provider = EmbeddingProviderFactory.get_provider()
            return await embedding_provider.generate_embedding(text)

        except HTTPException as http_exc:
            # Re-raise HTTP exceptions
            raise http_exc
        except Exception as e:
            logger.error(f"Error generating single embedding: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate embedding.",
            )

    @property
    def dimension(self) -> int:
        """Return the dimension of the embedding model"""
        return self._dimension

    def cosine_similarity(
        self, embedding1: List[float], embedding2: List[float]
    ) -> float:
        """
        Calculate cosine similarity between two embedding vectors.

        Args:
            embedding1: First embedding vector
            embedding2: Second embedding vector

        Returns:
            Cosine similarity score (0-1, where 1 is most similar)
        """
        try:
            if not embedding1 or not embedding2:
                return 0.0

            # Convert to numpy arrays
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)

            # Calculate cosine similarity
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            sim = dot_product / (norm1 * norm2)

            # Constrain to 0-1 range (handle floating point precision)
            return max(0.0, min(1.0, sim))

        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {str(e)}")
            return 0.0


# Global embedding service instance
_embedding_service = None


def get_embedding_service() -> EmbeddingService:
    """Get or create an EmbeddingService instance."""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service


# Example Usage (for testing)
async def _test_embedding_service():
    """Test the embedding service with the chosen provider"""
    import asyncio

    logging.basicConfig(level=logging.INFO)

    # Basic configuration check
    if not settings.OPENAI_API_KEY:
        logger.error("OPENAI_API_KEY not set. Cannot run the embedding test.")
        return

    logger.info("Testing the embedding service...")

    # Create the embedding service
    service = get_embedding_service()
    logger.info(f"Using embedding service model: {service.model}")
    logger.info(f"Embedding dimension: {service.dimension}")

    # Test text chunking
    test_text = "This is the first sentence. This is the second sentence. The third sentence is slightly longer."
    chunks = chunk_text(test_text)
    logger.info(f"Chunked text into {len(chunks)} chunks: {chunks}")

    # Test embedding generation
    if chunks:
        try:
            # Generate embeddings for chunks
            embeddings = await service.generate_embeddings(chunks)
            logger.info(f"Generated {len(embeddings)} embeddings.")

            # Verify embedding dimension
            if embeddings:
                logger.info(f"First embedding dimension: {len(embeddings[0])}")
                assert (
                    len(embeddings[0]) == service.dimension
                ), "Embedding dimension mismatch"

            # Generate a single embedding
            single_embedding = await service.generate_single_embedding(
                "A single test query."
            )
            logger.info(f"Single embedding dimension: {len(single_embedding)}")
            assert (
                len(single_embedding) == service.dimension
            ), "Single embedding dimension mismatch"

            logger.info("Embedding generation tests passed successfully.")
        except Exception as e:
            logger.error(f"Embedding test failed: {e}")
    
    logger.info("Embedding Service Test Completed.")


# To run test: python -m app.services.embedding_service
if __name__ == "__main__":
    import asyncio

    # Ensure the event loop policy is set correctly for Windows if needed
    # if os.name == 'nt':
    #     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(_test_embedding_service())
