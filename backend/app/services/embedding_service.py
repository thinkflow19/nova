import os
import openai
import pinecone
import importlib
from dotenv import load_dotenv
from fastapi import HTTPException, status
import uuid
import requests
import tempfile
import PyPDF2
import docx
from typing import List, Dict, Optional, Any, Type
import numpy as np
import logging
from enum import Enum
from abc import ABC, abstractmethod

# Import mock service
try:
    from app.services.mock_openai_service import MockEmbedding

    MOCK_SERVICE_AVAILABLE = True
except ImportError:
    MOCK_SERVICE_AVAILABLE = False
    print("Mock service not available - will not provide fallback for API issues.")

# Consider adding a text splitting library like langchain or nltk if complex chunking is needed
# from langchain.text_splitter import RecursiveCharacterTextSplitter

logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()


# Embedding Provider Enum
class EmbeddingProvider(str, Enum):
    OPENAI = "openai"
    PINECONE = "pinecone"
    # Add other providers as needed
    # HUGGINGFACE = "huggingface"
    # COHERE = "cohere"


# Global configuration
EMBEDDING_PROVIDER = os.getenv("EMBEDDING_PROVIDER", EmbeddingProvider.OPENAI)
EMBEDDING_MODEL = os.getenv(
    "EMBEDDING_MODEL", "text-embedding-ada-002"
)  # Default to OpenAI's ada model
EMBEDDING_DIMENSION = 1024  # Default dimension for most embedding models

# OpenAI configuration
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Pinecone configuration
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")
PINECONE_DIMENSION = 1024  # Dimensions for embeddings


# Abstract Embedding Provider Interface
class EmbeddingProviderInterface(ABC):
    """Abstract base class for embedding providers"""

    @abstractmethod
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        pass

    @abstractmethod
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        """Return the dimension of embeddings generated by this provider"""
        pass


# OpenAI Embedding Provider Implementation
class OpenAIEmbeddingProvider(EmbeddingProviderInterface):
    """OpenAI embedding provider implementation"""

    def __init__(self, model: str = EMBEDDING_MODEL):
        """Initialize with OpenAI configuration"""
        self.model = model
        self.api_key = OPENAI_API_KEY
        if not self.api_key:
            logger.error("OpenAI API key is not configured!")

        # Set the API key for the client
        openai.api_key = self.api_key
        logger.info(f"OpenAI embedding provider initialized with model: {self.model}")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI API"""
        if not texts:
            return []

        if not self.api_key:
            logger.error(
                "Cannot generate embeddings: OpenAI API key is not configured."
            )
            raise ValueError("OpenAI API key not configured.")

        try:
            logger.info(
                f"Generating embeddings for {len(texts)} chunks using {self.model}..."
            )
            try:
                # The new OpenAI SDK doesn't require awaiting the response
                response = openai.embeddings.create(input=texts, model=self.model)
                embeddings = [item.embedding for item in response.data]
                logger.info(f"Successfully generated {len(embeddings)} embeddings.")
            except Exception as e:
                if "insufficient_quota" in str(e) and MOCK_SERVICE_AVAILABLE:
                    logger.warning(
                        f"OpenAI API quota exceeded. Using mock embeddings instead."
                    )
                    # Generate mock embeddings that don't require await
                    mock_response = await MockEmbedding.create(
                        input=texts, model=self.model
                    )
                    embeddings = [item.embedding for item in mock_response.data]
                    logger.info(
                        f"Successfully generated {len(embeddings)} mock embeddings."
                    )
                else:
                    logger.error(f"Failed to generate OpenAI embeddings: {e}")
                    raise

            # Resize embeddings to match Pinecone's dimension if needed
            if embeddings and len(embeddings[0]) != PINECONE_DIMENSION:
                logger.warning(
                    f"Resizing embeddings from {len(embeddings[0])} to {PINECONE_DIMENSION} dimensions to match Pinecone"
                )
                resized_embeddings = []
                for emb in embeddings:
                    if len(emb) > PINECONE_DIMENSION:
                        # Truncate to first PINECONE_DIMENSION elements
                        resized_embeddings.append(emb[:PINECONE_DIMENSION])
                    else:
                        # Pad with zeros if needed (shouldn't happen with OpenAI models)
                        resized_embeddings.append(
                            emb + [0.0] * (PINECONE_DIMENSION - len(emb))
                        )
                return resized_embeddings

            return embeddings
        except Exception as e:
            logger.error(f"Failed to generate OpenAI embeddings: {e}")
            raise

    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        if not text:
            return []

        try:
            # The new OpenAI SDK doesn't require awaiting the response
            response = openai.embeddings.create(input=[text], model=self.model)
            embedding = response.data[0].embedding

            # Resize embedding to match Pinecone's dimension if needed
            if len(embedding) != PINECONE_DIMENSION:
                logger.warning(
                    f"Resizing embedding from {len(embedding)} to {PINECONE_DIMENSION} dimensions"
                )
                if len(embedding) > PINECONE_DIMENSION:
                    # Truncate
                    return embedding[:PINECONE_DIMENSION]
                else:
                    # Pad with zeros
                    return embedding + [0.0] * (PINECONE_DIMENSION - len(embedding))

            return embedding
        except (
            openai.RateLimitError,
            openai.APIStatusError,
            openai.APIConnectionError,
        ) as api_err:
            logger.error(
                f"OpenAI API error during embedding generation: {type(api_err).__name__}: {str(api_err)}"
            )
            # Check for quota error and use mock if available AND enabled
            if MOCK_SERVICE_AVAILABLE and (
                isinstance(api_err, openai.RateLimitError)
                or (
                    isinstance(api_err, openai.APIStatusError)
                    and "insufficient_quota" in str(api_err).lower()
                )
            ):

                logger.warning(
                    "OpenAI API quota/rate limit exceeded. Using mock embedding."
                )
                try:
                    mock_response = await MockEmbedding.create(
                        input=[text], model=self.model
                    )
                    # Ensure mock embedding dimension matches
                    mock_embedding = mock_response.data[0].embedding
                    if len(mock_embedding) != PINECONE_DIMENSION:
                        logger.warning(
                            f"Resizing mock embedding to {PINECONE_DIMENSION}"
                        )
                        if len(mock_embedding) > PINECONE_DIMENSION:
                            return mock_embedding[:PINECONE_DIMENSION]
                        else:
                            return mock_embedding + [0.0] * (
                                PINECONE_DIMENSION - len(mock_embedding)
                            )
                    return mock_embedding
                except Exception as mock_e:
                    logger.error(f"Failed to use mock embedding service: {mock_e}")
                    # If mock fails, re-raise the original API error
                    raise api_err from mock_e
            else:
                # For other API errors, re-raise
                raise
        except Exception as e:
            logger.error(
                f"Unexpected error generating embedding: {str(e)}", exc_info=True
            )
            raise

    @property
    def dimension(self) -> int:
        """Return dimension of embeddings for this provider, set to match Pinecone"""
        # Always return Pinecone dimension regardless of the actual model dimension
        return PINECONE_DIMENSION


# Factory for creating embedding providers
class EmbeddingProviderFactory:
    """Factory to create the appropriate embedding provider"""

    @staticmethod
    def get_provider(
        provider_type: EmbeddingProvider = None,
    ) -> EmbeddingProviderInterface:
        """Get the appropriate embedding provider based on configuration"""
        # Use provided type or fall back to env variable
        if provider_type is None:
            provider_type = EMBEDDING_PROVIDER

        try:
            provider_type = EmbeddingProvider(provider_type)
        except ValueError:
            logger.warning(
                f"Unknown embedding provider: {provider_type}. Falling back to OpenAI."
            )
            provider_type = EmbeddingProvider.OPENAI

        # Return appropriate provider
        if provider_type == EmbeddingProvider.OPENAI:
            return OpenAIEmbeddingProvider(model=EMBEDDING_MODEL)
        elif provider_type == EmbeddingProvider.PINECONE:
            # TODO: Implement Pinecone embedding provider
            # For now, fall back to OpenAI as Pinecone doesn't have its own embeddings yet
            logger.warning(
                "Pinecone embedding provider not yet implemented. Using OpenAI."
            )
            return OpenAIEmbeddingProvider(model=EMBEDDING_MODEL)
        else:
            # Default to OpenAI
            return OpenAIEmbeddingProvider(model=EMBEDDING_MODEL)


# Initialize Pinecone for vector storage (not embeddings)
try:
    # Reload pinecone module to ensure we have the latest version
    importlib.reload(pinecone)

    # Only initialize Pinecone if we have the required env vars
    if PINECONE_API_KEY and PINECONE_ENVIRONMENT and PINECONE_INDEX_NAME:
        logger.info(
            f"Initializing Pinecone for vector storage in {PINECONE_ENVIRONMENT}"
        )
        pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY)

        # Check if index exists
        indexes = pinecone_client.list_indexes()
        index_exists = any(idx.name == PINECONE_INDEX_NAME for idx in indexes)

        if not index_exists:
            logger.info(
                f"Creating Pinecone index: {PINECONE_INDEX_NAME} with dimension: {PINECONE_DIMENSION}"
            )
            pinecone_client.create_index(
                name=PINECONE_INDEX_NAME,
                dimension=PINECONE_DIMENSION,
                metric="cosine",
                spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
            )
            logger.info(f"Created Pinecone index: {PINECONE_INDEX_NAME}")

        # Connect to the index
        vector_store = pinecone_client.Index(PINECONE_INDEX_NAME)
        logger.info(f"✅ Connected to Pinecone index: {PINECONE_INDEX_NAME}")
    else:
        logger.warning(
            "Missing Pinecone configuration. Vector storage will not be available."
        )
        vector_store = None

except Exception as e:
    logger.warning(f"⚠️ Pinecone initialization failed: {str(e)}")
    logger.warning("⚠️ Vector storage will not be available")
    vector_store = None


def extract_text_from_file(file_url: str, file_name: str) -> str:
    """Extract text from uploaded documents (PDF, DOCX, TXT)."""
    try:
        # Download file from S3
        response = requests.get(file_url)
        if response.status_code != 200:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to download file from storage",
            )

        # Create temp file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(response.content)
            temp_file_path = temp_file.name

        # Extract text based on file type
        file_extension = file_name.split(".")[-1].lower()

        if file_extension == "pdf":
            # Process PDF
            text = ""
            with open(temp_file_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"

        elif file_extension == "docx":
            # Process DOCX
            doc = docx.Document(temp_file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])

        elif file_extension == "txt":
            # Process TXT
            with open(temp_file_path, "r", encoding="utf-8") as file:
                text = file.read()

        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Unsupported file type: {file_extension}",
            )

        # Clean up temp file
        os.unlink(temp_file_path)

        return text

    except Exception as e:
        if isinstance(e, HTTPException):
            raise e
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to extract text: {str(e)}",
        )


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks."""
    chunks = []

    if len(text) <= chunk_size:
        chunks.append(text)
    else:
        start = 0
        while start < len(text):
            end = min(start + chunk_size, len(text))
            if end < len(text) and text[end] != " ":
                # Try to end at a space to avoid breaking words
                while end > start and text[end] != " ":
                    end -= 1
                if end == start:  # If no space found, just use the hard cut
                    end = min(start + chunk_size, len(text))

            chunks.append(text[start:end])
            start = end - overlap  # Overlap for context

    return chunks


async def create_embeddings(text_chunks: List[str], metadata: Dict) -> str:
    """Create embeddings and store in Pinecone."""
    if not vector_store:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Vector storage is not available",
        )

    try:
        embedding_provider = EmbeddingProviderFactory.get_provider()
        embeddings_to_upsert = []

        for i, chunk in enumerate(text_chunks):
            vector_id = f"{metadata['document_id']}_{i}"
            try:
                embedding = await embedding_provider.generate_embedding(chunk)

                # Prepare metadata for this chunk
                chunk_metadata = metadata.copy()
                chunk_metadata["chunk_id"] = i
                chunk_metadata["text"] = chunk  # Storing original text chunk

                embeddings_to_upsert.append((vector_id, embedding, chunk_metadata))

            except Exception as e:
                logger.error(
                    f"Failed to generate embedding for chunk {i} (ID: {vector_id}): {e}. Skipping this chunk."
                )
                # Do NOT fallback to random embedding - skip this chunk instead
                continue

        if not embeddings_to_upsert:
            logger.warning(
                "No embeddings were generated successfully. Nothing to upsert."
            )
            # Depending on desired behavior, maybe raise an error here?
            # return None # Or raise an error
            raise ValueError(
                "Failed to generate any embeddings for the provided text chunks."
            )

        # Upsert embeddings to Pinecone
        logger.info(f"Upserting {len(embeddings_to_upsert)} vectors to Pinecone")
        result = vector_store.upsert(vectors=embeddings_to_upsert)
        logger.info(f"Upsert result: {result}")

        return metadata["document_id"]

    except pinecone.ApiException as pinecone_err:
        logger.error(f"Pinecone API error during upsert: {pinecone_err}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to store document embeddings in vector database.",
        )
    except ValueError as ve:
        # Catch the error raised if no embeddings were generated
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(ve),
        )
    except Exception as e:
        logger.error(f"Unexpected error creating embeddings: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An unexpected error occurred while processing the document.",
        )


async def query_embeddings(
    query: str, top_k: int = 3, project_id: str = None
) -> List[Dict]:
    """Query embeddings from Pinecone."""
    if not vector_store:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Vector storage is not available",
        )

    try:
        embedding_provider = EmbeddingProviderFactory.get_provider()

        try:
            query_embedding = await embedding_provider.generate_embedding(query)
        except Exception as e:
            logger.error(
                f"Failed to generate embedding for query '{query[:50]}...': {e}. Cannot perform search."
            )
            # Do NOT fallback to random embedding. Return empty results or raise error.
            # Option 1: Return empty list
            # return []
            # Option 2: Raise an exception to indicate failure
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to generate query embedding. Search unavailable.",
            ) from e

        filter_dict = {}
        if project_id:
            filter_dict["project_id"] = project_id

        logger.info(
            f"Querying Pinecone with filter: {filter_dict if filter_dict else 'None'}"
        )
        query_response = vector_store.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter=filter_dict if filter_dict else None,
        )

        results = []
        for match in query_response.matches:
            # ... (existing result processing) ...
            # Ensure metadata and text are present
            metadata = (
                match.metadata
                if hasattr(match, "metadata")
                else match.get("metadata", {})
            )
            text = metadata.get("text", None)
            if not metadata or not text:
                logger.warning(
                    f"Skipping match {match.id} due to missing metadata or text."
                )
                continue
            results.append(
                {
                    "text": text,
                    "score": (
                        match.score
                        if hasattr(match, "score")
                        else match.get("score", 0)
                    ),
                    "document_id": metadata.get("document_id", "Unknown"),
                    "file_name": metadata.get("file_name", "Unknown"),
                }
            )

        return results

    except pinecone.ApiException as pinecone_err:
        logger.error(f"Pinecone API error during query: {pinecone_err}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Failed to query vector database.",
        ) from pinecone_err
    except (
        HTTPException
    ) as http_exc:  # Re-raise HTTP exceptions from embedding generation
        raise http_exc
    except Exception as e:
        logger.error(f"Unexpected error querying embeddings: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred during search.",
        )


class EmbeddingService:
    def __init__(self, chunk_size=1000, chunk_overlap=100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        # Initialize the embedding provider
        self.provider = EmbeddingProviderFactory.get_provider()
        logger.info(
            f"EmbeddingService initialized with provider: {self.provider.__class__.__name__}, "
            f"dimension: {self.provider.dimension}"
        )

    def _chunk_text(self, text: str) -> List[str]:
        """Simple text chunking based on paragraphs or fixed size."""
        # Replace with more sophisticated chunking if needed
        # E.g., using RecursiveCharacterTextSplitter
        # text_splitter = RecursiveCharacterTextSplitter(
        #     chunk_size=self.chunk_size,
        #     chunk_overlap=self.chunk_overlap
        # )
        # return text_splitter.split_text(text)

        # Basic fixed-size chunking (non-overlapping)
        chunks = []
        if not text:
            return chunks
        for i in range(0, len(text), self.chunk_size):
            chunks.append(text[i : i + self.chunk_size])

        logger.info(f"Chunked text into {len(chunks)} chunks.")
        return chunks

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of text chunks."""
        if not texts:
            return []

        try:
            logger.info(f"Generating embeddings for {len(texts)} chunks")
            # The provider might return awaitable or not, so handle both cases
            try:
                return await self.provider.generate_embeddings(texts)
            except (TypeError, ValueError) as e:
                if "can't be used in 'await' expression" in str(e):
                    # If the provider doesn't need to be awaited
                    return self.provider.generate_embeddings(texts)
                raise
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise

    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single piece of text."""
        if not text:
            return []

        try:
            # The provider might return awaitable or not, so handle both cases
            try:
                return await self.provider.generate_embedding(text)
            except (TypeError, ValueError) as e:
                if "can't be used in 'await' expression" in str(e):
                    # If the provider doesn't need to be awaited
                    return self.provider.generate_embedding(text)
                raise
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            raise

    def get_dimension(self) -> int:
        """Return the dimension of the embeddings generated by this service."""
        return self.provider.dimension


# Example Usage (for testing)
async def _test_embedding_service():
    """Test the embedding service with the chosen provider"""
    import asyncio

    logging.basicConfig(level=logging.INFO)

    # Basic configuration check
    if (
        not os.getenv("OPENAI_API_KEY")
        and EMBEDDING_PROVIDER == EmbeddingProvider.OPENAI
    ):
        logger.warning(
            "Skipping test: OPENAI_API_KEY not set but OpenAI provider selected."
        )
        return

    logger.info("Testing the embedding service...")

    # Create the embedding service
    service = EmbeddingService()
    logger.info(f"Using embedding provider: {service.provider.__class__.__name__}")
    logger.info(f"Embedding dimension: {service.provider.dimension}")

    # Test text chunking
    test_text = "This is the first sentence. This is the second sentence. The third sentence is slightly longer."
    chunks = service._chunk_text(test_text)
    logger.info(f"Chunked text into {len(chunks)} chunks: {chunks}")

    # Test embedding generation
    if chunks:
        try:
            # Generate embeddings for chunks
            embeddings = await service.generate_embeddings(chunks)
            logger.info(f"Generated {len(embeddings)} embeddings.")

            # Verify embedding dimension
            if embeddings:
                logger.info(f"First embedding dimension: {len(embeddings[0])}")
                assert (
                    len(embeddings[0]) == service.get_dimension()
                ), "Embedding dimension mismatch"

            # Generate a single embedding
            single_embedding = await service.generate_embedding("A single test query.")
            logger.info(f"Single embedding dimension: {len(single_embedding)}")
            assert (
                len(single_embedding) == service.get_dimension()
            ), "Single embedding dimension mismatch"

            logger.info("Embedding generation tests passed successfully.")
        except Exception as e:
            logger.error(f"Embedding test failed: {e}")

    # Test vector store if available
    if vector_store:
        try:
            logger.info("Testing vector store operations...")

            # Create a test document with random ID
            test_id = f"test-{uuid.uuid4().hex[:8]}"
            metadata = {
                "document_id": test_id,
                "file_name": "test.txt",
                "project_id": "test-project",
            }

            # Store the test text chunks
            await create_embeddings(chunks, metadata)
            logger.info("Successfully stored test vectors")

            # Query the vectors
            results = await query_embeddings(
                query="This is a test query about sentences.",
                top_k=2,
                project_id="test-project",
            )

            logger.info(f"Query returned {len(results)} results: {results}")

            logger.info("Vector store tests completed successfully.")
        except Exception as e:
            logger.error(f"Vector store test failed: {e}")
    else:
        logger.warning("Vector store is not available, skipping related tests.")

    logger.info("Embedding Service Test Completed.")


# To run test: python -m app.services.embedding_service
if __name__ == "__main__":
    import asyncio

    # Ensure the event loop policy is set correctly for Windows if needed
    # if os.name == 'nt':
    #     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(_test_embedding_service())
