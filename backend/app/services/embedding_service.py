import os
import openai
from pinecone import Pinecone
import importlib
import uuid
import requests
import tempfile
import PyPDF2
import docx
from typing import List, Dict, Optional, Any, Type, Union
import numpy as np
import logging
from enum import Enum
from abc import ABC, abstractmethod
import httpx
import asyncio
from fastapi import HTTPException, status
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from openai import AsyncOpenAI

# Import settings instance from centralized config module
from app.config.settings import settings

# Configure logging
logger = logging.getLogger(__name__)

# Initialize OpenAI client using settings
openai_client = AsyncOpenAI(
    api_key=settings.OPENAI_API_KEY,
    base_url=settings.OPENAI_API_BASE,  # Use the full base URL from settings
)
logger.info(f"OpenAI client initialized with base URL: {settings.OPENAI_API_BASE}")

# Initialize Pinecone for vector storage (not embeddings)
try:
    # Only initialize Pinecone if we have the required env vars
    if settings.PINECONE_API_KEY and settings.PINECONE_ENVIRONMENT:
        logger.info(
            f"Initializing Pinecone for vector storage in {settings.PINECONE_ENVIRONMENT}"
        )
        # Initialize Pinecone with the latest API
        pc = Pinecone(api_key=settings.PINECONE_API_KEY)

        # Use settings.PINECONE_INDEX if available, otherwise default to 'proj'
        index_name = settings.PINECONE_INDEX or "proj"
        
        # Check if index exists
        existing_indexes = pc.list_indexes().names()
        
        if index_name not in existing_indexes:
            logger.info(
                f"Creating Pinecone index: {index_name} with dimension: {settings.EMBEDDING_DIMENSION}"
            )
            pc.create_index(
                name=index_name,
                dimension=settings.EMBEDDING_DIMENSION,
                metric="cosine",
                spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
            )
            logger.info(f"Created Pinecone index: {index_name}")

        # Connect to the index
        vector_store = pc.Index(index_name)
        # Store the Pinecone client for other services to use
        pinecone_client = pc
        logger.info(f"✅ Connected to Pinecone index: {index_name}")
    else:
        logger.warning(
            "Missing Pinecone configuration. Vector storage will not be available."
        )
        vector_store = None
        pinecone_client = None

except Exception as e:
    logger.warning(f"⚠️ Pinecone initialization failed: {str(e)}")
    logger.warning("⚠️ Vector storage will not be available")
    vector_store = None
    pinecone_client = None


# Embedding Provider Enum
class EmbeddingProvider(str, Enum):
    OPENAI = "openai"
    PINECONE = "pinecone"
    # Add other providers as needed
    # HUGGINGFACE = "huggingface"
    # COHERE = "cohere"


# Abstract Embedding Provider Interface
class EmbeddingProviderInterface(ABC):
    """Abstract base class for embedding providers"""

    @abstractmethod
    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts"""
        pass

    @abstractmethod
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        """Return the dimension of embeddings generated by this provider"""
        pass


# OpenAI Embedding Provider Implementation
class OpenAIEmbeddingProvider(EmbeddingProviderInterface):
    """OpenAI embedding provider implementation"""

    def __init__(self, model: str = settings.EMBEDDING_MODEL):
        """Initialize with OpenAI configuration"""
        self.model = model
        self.api_key = settings.OPENAI_API_KEY
        self.api_base = settings.OPENAI_API_BASE

        if not self.api_key:
            logger.error("OpenAI API key is not configured!")
            raise ValueError(
                "OpenAI API key not configured. Cannot generate embeddings."
            )

        # Set the API key and base URL for the client
        openai.api_key = self.api_key
        openai.base_url = self.api_base

        logger.info(f"OpenAI embedding provider initialized with model: {self.model}")
        logger.info(f"Using API base URL: {self.api_base}")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings using OpenAI API"""
        if not texts:
            return []

        try:
            logger.info(
                f"Generating embeddings for {len(texts)} chunks using {self.model}..."
            )

            # The new OpenAI SDK doesn't require awaiting the response
            response = openai.embeddings.create(input=texts, model=self.model)
            embeddings = [item.embedding for item in response.data]
            logger.info(f"Successfully generated {len(embeddings)} embeddings.")

            # Resize embeddings to match Pinecone's dimension if needed
            if embeddings and len(embeddings[0]) != settings.EMBEDDING_DIMENSION:
                logger.warning(
                    f"Resizing embeddings from {len(embeddings[0])} to {settings.EMBEDDING_DIMENSION} dimensions to match Pinecone"
                )
                resized_embeddings = []
                for emb in embeddings:
                    if len(emb) > settings.EMBEDDING_DIMENSION:
                        # Truncate to first EMBEDDING_DIMENSION elements
                        resized_embeddings.append(emb[: settings.EMBEDDING_DIMENSION])
                    else:
                        # Pad with zeros if needed (shouldn't happen with OpenAI models)
                        resized_embeddings.append(
                            emb + [0.0] * (settings.EMBEDDING_DIMENSION - len(emb))
                        )
                return resized_embeddings

            return embeddings

        except openai.RateLimitError as rate_err:
            logger.error(f"OpenAI rate limit exceeded: {rate_err}")
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="API rate limit exceeded. Please try again later.",
            )
        except openai.APIError as api_err:
            logger.error(f"OpenAI API error: {api_err}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Error connecting to embedding service. Please try again later.",
            )
        except Exception as e:
            logger.error(f"Failed to generate OpenAI embeddings: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate embeddings.",
            )

    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text"""
        if not text:
            return []

        try:
            # The new OpenAI SDK doesn't require awaiting the response
            response = openai.embeddings.create(input=[text], model=self.model)
            embedding = response.data[0].embedding

            # Resize embedding to match Pinecone's dimension if needed
            if len(embedding) != settings.EMBEDDING_DIMENSION:
                logger.warning(
                    f"Resizing embedding from {len(embedding)} to {settings.EMBEDDING_DIMENSION} dimensions"
                )
                if len(embedding) > settings.EMBEDDING_DIMENSION:
                    # Truncate
                    return embedding[: settings.EMBEDDING_DIMENSION]
                else:
                    # Pad with zeros
                    return embedding + [0.0] * (
                        settings.EMBEDDING_DIMENSION - len(embedding)
                    )

            return embedding

        except (openai.RateLimitError, openai.APIStatusError) as api_err:
            logger.error(
                f"OpenAI API error during embedding generation: {type(api_err).__name__}: {str(api_err)}"
            )
            if "insufficient_quota" in str(api_err).lower():
                raise HTTPException(
                    status_code=status.HTTP_402_PAYMENT_REQUIRED,
                    detail="OpenAI API quota exceeded. Please check your subscription.",
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Embedding service temporarily unavailable. Please try again later.",
                )
        except openai.APIConnectionError as conn_err:
            logger.error(f"OpenAI API connection error: {conn_err}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Could not connect to embedding service. Please try again later.",
            )
        except Exception as e:
            logger.error(
                f"Unexpected error generating embedding: {str(e)}", exc_info=True
            )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="An unexpected error occurred while generating embeddings.",
            )

    @property
    def dimension(self) -> int:
        """Return dimension of embeddings for this provider, set to match Pinecone"""
        # Always return Pinecone dimension regardless of the actual model dimension
        return settings.EMBEDDING_DIMENSION


# Factory for creating embedding providers
class EmbeddingProviderFactory:
    """Factory to create the appropriate embedding provider"""

    @staticmethod
    def get_provider(
        provider_type: EmbeddingProvider = None,
    ) -> EmbeddingProviderInterface:
        """Get the appropriate embedding provider based on configuration"""
        # Use provided type or fall back to env variable
        if provider_type is None:
            provider_type = settings.EMBEDDING_PROVIDER

        try:
            provider_type = EmbeddingProvider(provider_type)
        except ValueError:
            logger.warning(
                f"Unknown embedding provider: {provider_type}. Falling back to OpenAI."
            )
            provider_type = EmbeddingProvider.OPENAI

        # Return appropriate provider
        if provider_type == EmbeddingProvider.OPENAI:
            return OpenAIEmbeddingProvider(model=settings.EMBEDDING_MODEL)
        elif provider_type == EmbeddingProvider.PINECONE:
            # TODO: Implement Pinecone embedding provider
            # For now, fall back to OpenAI as Pinecone doesn't have its own embeddings yet
            logger.warning(
                "Pinecone embedding provider not yet implemented. Using OpenAI."
            )
            return OpenAIEmbeddingProvider(model=settings.EMBEDDING_MODEL)
        else:
            # Default to OpenAI
            return OpenAIEmbeddingProvider(model=settings.EMBEDDING_MODEL)


def extract_text_from_file(file_url: str, file_name: str) -> str:
    """Extract text from uploaded documents (PDF, DOCX, TXT)."""
    try:
        # Download file from S3
        response = requests.get(file_url)
        if response.status_code != 200:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to download file from storage",
            )

        # Create temp file
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(response.content)
            temp_file_path = temp_file.name

        # Extract text based on file type
        file_extension = file_name.split(".")[-1].lower()

        if file_extension == "pdf":
            # Process PDF
            text = ""
            with open(temp_file_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"

        elif file_extension == "docx":
            # Process DOCX
            doc = docx.Document(temp_file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])

        elif file_extension == "txt":
            # Process TXT
            with open(temp_file_path, "r", encoding="utf-8") as file:
                text = file.read()

        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Unsupported file type: {file_extension}",
            )

        # Clean up temp file
        os.unlink(temp_file_path)

        return text

    except Exception as e:
        if isinstance(e, HTTPException):
            raise e
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to extract text: {str(e)}",
        )


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """Split text into overlapping chunks."""
    chunks = []

    if len(text) <= chunk_size:
        chunks.append(text)
    else:
        start = 0
        while start < len(text):
            end = min(start + chunk_size, len(text))
            if end < len(text) and text[end] != " ":
                # Try to end at a space to avoid breaking words
                while end > start and text[end] != " ":
                    end -= 1
                if end == start:  # If no space found, just use the hard cut
                    end = min(start + chunk_size, len(text))

            chunks.append(text[start:end])
            start = end - overlap  # Overlap for context

    return chunks


async def create_embeddings(text_chunks: List[str], metadata: Dict) -> str:
    """Create embeddings and store in Pinecone."""
    if not vector_store:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Vector storage is not available",
        )

    try:
        embedding_provider = EmbeddingProviderFactory.get_provider()
        embeddings_to_upsert = []

        for i, chunk in enumerate(text_chunks):
            vector_id = f"{metadata['document_id']}_{i}"
            try:
                embedding = await embedding_provider.generate_embedding(chunk)

                # Prepare metadata for this chunk
                chunk_metadata = metadata.copy()
                chunk_metadata["chunk_id"] = i
                chunk_metadata["text"] = chunk  # Storing original text chunk

                embeddings_to_upsert.append((vector_id, embedding, chunk_metadata))

            except Exception as e:
                logger.error(
                    f"Failed to generate embedding for chunk {i} (ID: {vector_id}): {e}. Skipping this chunk."
                )
                # Skip this chunk and continue with others
                continue

        if not embeddings_to_upsert:
            logger.warning(
                "No embeddings were generated successfully. Nothing to upsert."
            )
            raise ValueError(
                "Failed to generate any embeddings for the provided text chunks."
            )

        # Upsert embeddings to Pinecone
        logger.info(f"Upserting {len(embeddings_to_upsert)} vectors to Pinecone")
        result = vector_store.upsert(vectors=embeddings_to_upsert)
        logger.info(f"Upsert result: {result}")

        return metadata["document_id"]

    except pinecone.ApiException as pinecone_err:
        logger.error(f"Pinecone API error during upsert: {pinecone_err}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to store document embeddings in vector database.",
        )
    except ValueError as ve:
        # Catch the error raised if no embeddings were generated
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(ve),
        )
    except Exception as e:
        logger.error(f"Unexpected error creating embeddings: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"An unexpected error occurred while processing the document.",
        )


async def query_embeddings(
    query: str, top_k: int = 3, project_id: str = None
) -> List[Dict]:
    """Query embeddings from Pinecone."""
    if not vector_store:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Vector storage is not available",
        )

    try:
        embedding_provider = EmbeddingProviderFactory.get_provider()

        try:
            query_embedding = await embedding_provider.generate_embedding(query)
        except Exception as e:
            logger.error(
                f"Failed to generate embedding for query '{query[:50]}...': {e}. Cannot perform search."
            )
            # Raise the exception to indicate failure
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to generate query embedding. Search unavailable.",
            ) from e

        filter_dict = {}
        if project_id:
            filter_dict["project_id"] = project_id

        logger.info(
            f"Querying Pinecone with filter: {filter_dict if filter_dict else 'None'}"
        )
        query_response = vector_store.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True,
            filter=filter_dict if filter_dict else None,
        )

        results = []
        for match in query_response.matches:
            # Ensure metadata and text are present
            metadata = (
                match.metadata
                if hasattr(match, "metadata")
                else match.get("metadata", {})
            )
            text = metadata.get("text", None)
            if not metadata or not text:
                logger.warning(
                    f"Skipping match {match.id} due to missing metadata or text."
                )
                continue
            results.append(
                {
                    "text": text,
                    "score": (
                        match.score
                        if hasattr(match, "score")
                        else match.get("score", 0)
                    ),
                    "document_id": metadata.get("document_id", "Unknown"),
                    "file_name": metadata.get("file_name", "Unknown"),
                }
            )

        return results

    except pinecone.ApiException as pinecone_err:
        logger.error(f"Pinecone API error during query: {pinecone_err}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Failed to query vector database.",
        ) from pinecone_err
    except (
        HTTPException
    ) as http_exc:  # Re-raise HTTP exceptions from embedding generation
        raise http_exc
    except Exception as e:
        logger.error(f"Unexpected error querying embeddings: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An unexpected error occurred during search.",
        )


class EmbeddingService:
    """Service for generating embeddings from text."""

    def __init__(self):
        """Initialize the embedding service using settings."""
        self.model = settings.EMBEDDING_MODEL
        self._dimension = settings.EMBEDDING_DIMENSION
        self.provider = settings.EMBEDDING_PROVIDER
        self.openai_client = openai_client  # Use the shared client
        self.pinecone_client = pinecone_client  # Use the shared client instance
        self.pinecone_index_name = settings.PINECONE_INDEX

        logger.info(f"Embedding service initialized with model: {self.model}")
        if self.provider == "openai":
            logger.info(f"Using embedding endpoint: {settings.OPENAI_EMBEDDINGS_URL}")
        if not self.pinecone_client:
            logger.warning("Pinecone client not available to EmbeddingService.")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of text strings.

        Args:
            texts: List of text strings to embed

        Returns:
            List of embedding vectors (one per input text)
        """
        if not texts:
            return []

        try:
            logger.info(f"Generating embeddings for {len(texts)} texts")

            # Split into batches of 100 (OpenAI limit)
            batch_size = 100
            text_batches = [
                texts[i : i + batch_size] for i in range(0, len(texts), batch_size)
            ]

            all_embeddings = []

            # Process each batch
            for i, batch in enumerate(text_batches):
                payload = {"model": self.model, "input": batch}

                async with httpx.AsyncClient(timeout=60.0) as client:
                    response = await client.post(
                        settings.OPENAI_EMBEDDINGS_URL,
                        headers={"Authorization": f"Bearer {settings.OPENAI_API_KEY}"},
                        json=payload,
                    )

                    if response.status_code != 200:
                        logger.error(
                            f"OpenAI API error: {response.status_code} - {response.text}"
                        )
                        raise HTTPException(
                            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                            detail=f"Embedding service error: {response.status_code}",
                        )

                    result = response.json()

                    # Extract and sort embeddings by index to maintain order
                    embeddings_batch = sorted(
                        result.get("data", []), key=lambda x: x.get("index", 0)
                    )

                    batch_vectors = [
                        item.get("embedding", []) for item in embeddings_batch
                    ]
                    all_embeddings.extend(batch_vectors)

                    logger.debug(
                        f"Generated {len(batch_vectors)} embeddings for batch {i+1}/{len(text_batches)}"
                    )

                    # Add a small delay between batches to avoid rate limiting
                    if i < len(text_batches) - 1:
                        await asyncio.sleep(0.5)

            logger.info(f"Successfully generated {len(all_embeddings)} embeddings")
            return all_embeddings

        except HTTPException as http_exc:
            # Re-raise HTTP exceptions
            raise http_exc
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate embeddings.",
            )

    async def generate_single_embedding(self, text: str) -> List[float]:
        """
        Generate an embedding for a single text string.

        Args:
            text: Text string to embed

        Returns:
            Embedding vector for the input text
        """
        try:
            logger.info("Generating single embedding")

            payload = {"model": self.model, "input": text}

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    settings.OPENAI_EMBEDDINGS_URL,
                    headers={"Authorization": f"Bearer {settings.OPENAI_API_KEY}"},
                    json=payload,
                )

                if response.status_code != 200:
                    logger.error(
                        f"OpenAI API error: {response.status_code} - {response.text}"
                    )
                    if response.status_code == 429:
                        raise HTTPException(
                            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                            detail="Rate limit exceeded on embedding service.",
                        )
                    else:
                        raise HTTPException(
                            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                            detail=f"Embedding service error: {response.status_code}",
                        )

                result = response.json()
                embedding = result.get("data", [{}])[0].get("embedding", [])

                logger.info("Successfully generated single embedding")
                return embedding

        except HTTPException as http_exc:
            # Re-raise HTTP exceptions
            raise http_exc
        except Exception as e:
            logger.error(f"Error generating single embedding: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to generate embedding.",
            )

    @property
    def dimension(self) -> int:
        """Return the dimension of the embedding model"""
        return self._dimension

    def cosine_similarity(
        self, embedding1: List[float], embedding2: List[float]
    ) -> float:
        """
        Calculate cosine similarity between two embedding vectors.

        Args:
            embedding1: First embedding vector
            embedding2: Second embedding vector

        Returns:
            Cosine similarity score (0-1, where 1 is most similar)
        """
        try:
            if not embedding1 or not embedding2:
                return 0.0

            # Convert to numpy arrays
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)

            # Calculate cosine similarity
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            sim = dot_product / (norm1 * norm2)

            # Constrain to 0-1 range (handle floating point precision)
            return max(0.0, min(1.0, sim))

        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {str(e)}")
            return 0.0


# Global embedding service instance
_embedding_service = None


def get_embedding_service() -> EmbeddingService:
    """Get or create an EmbeddingService instance."""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service


# Example Usage (for testing)
async def _test_embedding_service():
    """Test the embedding service with the chosen provider"""
    import asyncio

    logging.basicConfig(level=logging.INFO)

    # Basic configuration check
    if not settings.OPENAI_API_KEY:
        logger.error("OPENAI_API_KEY not set. Cannot run the embedding test.")
        return

    logger.info("Testing the embedding service...")

    # Create the embedding service
    service = get_embedding_service()
    logger.info(f"Using embedding service model: {service.model}")
    logger.info(f"Embedding dimension: {service.dimension}")

    # Test text chunking
    test_text = "This is the first sentence. This is the second sentence. The third sentence is slightly longer."
    chunks = chunk_text(test_text)
    logger.info(f"Chunked text into {len(chunks)} chunks: {chunks}")

    # Test embedding generation
    if chunks:
        try:
            # Generate embeddings for chunks
            embeddings = await service.generate_embeddings(chunks)
            logger.info(f"Generated {len(embeddings)} embeddings.")

            # Verify embedding dimension
            if embeddings:
                logger.info(f"First embedding dimension: {len(embeddings[0])}")
                assert (
                    len(embeddings[0]) == service.dimension
                ), "Embedding dimension mismatch"

            # Generate a single embedding
            single_embedding = await service.generate_single_embedding(
                "A single test query."
            )
            logger.info(f"Single embedding dimension: {len(single_embedding)}")
            assert (
                len(single_embedding) == service.dimension
            ), "Single embedding dimension mismatch"

            logger.info("Embedding generation tests passed successfully.")
        except Exception as e:
            logger.error(f"Embedding test failed: {e}")

    # Test vector store if available
    if vector_store:
        try:
            logger.info("Testing vector store operations...")

            # Create a test document with random ID
            test_id = f"test-{uuid.uuid4().hex[:8]}"
            metadata = {
                "document_id": test_id,
                "file_name": "test.txt",
                "project_id": "test-project",
            }

            # Store the test text chunks
            await create_embeddings(chunks, metadata)
            logger.info("Successfully stored test vectors")

            # Query the vectors
            results = await query_embeddings(
                query="This is a test query about sentences.",
                top_k=2,
                project_id="test-project",
            )

            logger.info(f"Query returned {len(results)} results: {results}")

            logger.info("Vector store tests completed successfully.")
        except Exception as e:
            logger.error(f"Vector store test failed: {e}")
    else:
        logger.warning("Vector store is not available, skipping related tests.")

    logger.info("Embedding Service Test Completed.")


# To run test: python -m app.services.embedding_service
if __name__ == "__main__":
    import asyncio

    # Ensure the event loop policy is set correctly for Windows if needed
    # if os.name == 'nt':
    #     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(_test_embedding_service())
